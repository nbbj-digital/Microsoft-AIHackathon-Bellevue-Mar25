# ğŸ† Challenge 1: Setting Up Fabric & Storage Solutions  

## ğŸ“– Scenario  
You need to implement a **cloud-based solution** for securely storing and managing financial transaction PDFs. This challenge focuses on **building the foundation** for data storage and access using **Microsoft Fabric and Azure Storage**.  

Your goal is to set up the **necessary infrastructure** to support a scalable data pipeline.  

> **Note:** If you **already completed the Fabric Capacity setup** and **Service Principal creation** as per the **prerequisites email**, you can **skip those steps** and proceed with the rest of the challenge.  

---

## ğŸ¯ Your Mission  
By completing this challenge, you will:  

âœ… Set up **Microsoft Fabric Capacity** *(skip if completed in prerequisites)*  
âœ… Create a **OneLake Lakehouse** to store financial transaction PDFs  
âœ… Download, unzip, and upload **financial data** to **OneLake**  
âœ… Create & Configure a **Service Principal** for authentication *(skip if completed in prerequisites)*  
âœ… Assign appropriate **permissions** in Fabric & Azure Storage  
âœ… Retrieve **Document Intelligence API endpoint & keys** for later use  

> **Reminder:** This challenge is about **building the data pipeline**. You will not process the documents yet.  

---

## ğŸš€ Step 1: Create Microsoft Fabric Capacity *(Skip if completed in prerequisites)*  
ğŸ’¡ **Why?** Microsoft Fabric provides the necessary **capacity to store and process large datasets**.  

### 1ï¸âƒ£ Create Fabric Capacity in Azure  
ğŸ”¹ Use the **Azure Portal** to create a **Fabric Capacity** resource.  

ğŸ”¹ **Things to consider:**  
   - What **SKU** is required for this challenge?  
   - Should **security features** like Private Link be enabled?  

âœ… **Best Practice**: Configure **RBAC (Role-Based Access Control)** to manage access.  

---

## ğŸš€ Step 2: Assign Fabric Capacity in Microsoft Fabric *(Skip if completed in prerequisites)*  
ğŸ’¡ **Why?** The Fabric workspace must be **assigned to a capacity** before you can use it.  

### 1ï¸âƒ£ Assign Fabric Capacity  
ğŸ”¹ In **Microsoft Fabric**, assign the **Fabric Capacity** to your workspace.  

ğŸ”¹ **Hint:** Where in the Fabric UI can you manage capacity assignments?  

âœ… **Outcome**: Your **Fabric workspace** should now be connected to a **Fabric Capacity**.  

---

## ğŸš€ Step 3: Create a OneLake Lakehouse  
ğŸ’¡ **Why?** A **OneLake Lakehouse** is needed to store **unprocessed PDFs** before AI processing.  

### 1ï¸âƒ£ Create a Fabric Lakehouse  
ğŸ”¹ In **Microsoft Fabric**, create a **new Lakehouse** inside your workspace.  

ğŸ”¹ **Hint:** What folder structure would be best for organizing financial PDFs?  

âœ… **Best Practice**: Keep a **structured folder hierarchy** for better organization.  

---

## ğŸš€ Step 4: Download & Upload Financial Data to OneLake  
ğŸ’¡ **Why?** Your **AI models** need structured **financial PDFs** to analyze.  

### 1ï¸âƒ£ Download and Extract the Financial Data  
ğŸ”¹ Download the **financial data ZIP file** from the following link:  
   ğŸ”— [Financial Data.zip](https://github.com/DavidArayaS/AI-Powered-Insights-Fraud-Detection-Hackathon/blob/05f385b28e7d4215b0e1bf52eeaabf63b70e7c1a/Data%20Sources/Financial%20Data.zip)  

ğŸ”¹ Extract the **ZIP file** on your local machine.  
ğŸ”¹ Locate the extracted **Financial Data** folder containing the transaction PDFs.  

### 2ï¸âƒ£ Upload Financial Data to OneLake  
ğŸ”¹ Open **Microsoft Fabric** â†’ Navigate to **YourLakehouse**.  
ğŸ”¹ Click on **Files** (inside the Lakehouse).  
ğŸ”¹ Click **Upload Folder** â†’ Select the extracted Folder with the **PDF files**.  

![alt text](https://github.com/DavidArayaS/AI-Powered-Insights-Fraud-Detection-Hackathon/blob/f6b939b949003bbd1655cd718a81b2852f2d4c51/01-Data%20Ingestion/Reference%20Pictures/%7BB587EEAC-7F75-4991-9FCF-98F72CEF18CF%7D.png)

âœ… **Outcome**: Your **financial data** is now **organized and available** in OneLake.  

---

## ğŸš€ Step 5: Create a Storage Account  
ğŸ’¡ **Why?** This **Storage Account** will be used for **future data transfers from Fabric**.  

### 1ï¸âƒ£ Create an Azure Storage Account  
ğŸ”¹ Use the **Azure Portal** to create a **Storage Account**.  

ğŸ”¹ **Things to consider:**  
   - What replication type is best for performance and security?  

âœ… **Outcome**: Your **Storage Account** is now ready for future **data movement**.  

---

## ğŸš€ Step 6: Create & Configure a Service Principal *(Skip if completed in prerequisites)*  
ğŸ’¡ **Why?** A **Service Principal** allows secure, automated authentication between services.  

### 1ï¸âƒ£ Create a Service Principal in Azure Entra ID  
ğŸ”¹ In **Azure Entra ID (Active Directory)**, register a **new application** for the **Service Principal**.  

âœ… **Outcome**: Your **Service Principal** is registered but still needs **permissions**.  

---

## ğŸš€ Step 7: Generate & Store Service Principal Credentials *(Skip if completed in prerequisites)*  
ğŸ’¡ **Why?** The **Service Principal** needs **credentials** to authenticate.  

### 1ï¸âƒ£ Create a Client Secret  
ğŸ”¹ Generate a **Client Secret** and **securely store** its value.  

âœ… **Outcome**: You now have the **Client ID, Tenant ID, and Secret Key** for authentication.  

---

## ğŸš€ Step 8: Assign Permissions in Fabric & Blob Storage  
ğŸ’¡ **Why?** The **Service Principal** must be granted **access** to Fabric and Azure Storage.  

âœ… **Outcome**: The **Service Principal now has full access** to **Fabric & Blob Storage**.  

---

## ğŸš€ Step 9: Create & Retrieve Document Intelligence Service Endpoint & Keys  
ğŸ’¡ **Why?** The **Document Intelligence API** will be used later to process financial PDFs.  

ğŸ”¹ **Steps to retrieve the API endpoint and keys**:  
   1. Go to **Azure Portal** â†’ Search for **Document Intelligence**.  
   2. Select your **Document Intelligence instance**.  
   3. Click on **Keys and Endpoint** in the left menu.  
   4. **Copy & Save** the following:  
      - **Endpoint URL**  
      - **Primary Key**  

âœ… **Outcome**: You now have **authentication details** to use **Document Intelligence API** later.  

---

## ğŸš€ Step 10: Create a Standard LRS Storage Account  
ğŸ’¡ **Why?** This **Storage Account** will be used to store **analyzed JSON data**.  


âœ… **Outcome**: The **Storage Account** is ready for storing processed data.  

[Supporting Document](https://learn.microsoft.com/en-us/fabric/onelake/onelake-azure-storage-explorer)

---

## ğŸ Final Challenge Checkpoints  
âœ… Is **Fabric Capacity** assigned correctly? *(Skip if already done in prerequisites)*  
âœ… Do all **financial PDFs** appear in **OneLake** under **/Files/FinancialData/**?  
âœ… Do both **Storage Accounts** and the **Document Intelligence Service** exist and are they ready for use?  
âœ… Is your **Service Principal configured** with the necessary **permissions**?

Once all steps are completed, you are ready to move on to **Challenge 2! ğŸš€**  

---

## â“ Troubleshooting Tips  
ğŸ”¹ If **Fabric** does not recognize your **Service Principal**, check if **API access** is enabled.  
ğŸ”¹ If you cannot **upload files to OneLake**, verify the assigned **capacity and workspace settings**.  
ğŸ”¹ If the **Storage Account** is not accessible, ensure the correct **RBAC roles** are assigned.  
ğŸ”¹ If the **Document Intelligence API** is not working, verify the **endpoint and keys** are correct.  
